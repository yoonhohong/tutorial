---
title: "데이터 임상의학 w/ R (5)"
date: Aug 28, 2018
author: "Yoon-Ho Hong, MD, PhD"
output:
  html_document:
    toc: yes
    toc_float: true
---

# 회귀(regression)

fit a simple linear regression model, with medv as the response and lstat as the predictor.

- medv; median house value
- lstat; percent of households with low socioeconomic status
<br>

## Questions 

### 1. Is there relationship between the median house value and the percent of households with low socioeconomic status? 

1. fit a linear model 
2. test the hypothesis: $\beta_x$ = 0 

```{r}
library(MASS)
attach(Boston)
lm.fit = lm(medv ~ lstat)
lm.fit
summary(lm.fit)
```
<br> 

***   
**Note**  
1. How to calculate the estimates of parameters?  
2. How large is the effect of percent of households with low socioeconomic status on median house value?   

***   

```{r}
names(lm.fit)
coef(lm.fit)
confint(lm.fit)
```
<br>

### 2. How accurate is the linear model? 

1. R-squared (percentage of variability in the response that is explained by the predictors) 

$$R^2 = \frac{TSS - RSS}{TSS}$$

- TSS: Total sum of squares 
- RSS: Residual sum of squares

2. Root mean squared error.  

```{r}
rmse = function(x){
  sqrt(sum(residuals(x)^2)/df.residual(lm.fit))
}
```


***   
**Note** *How different is the multiple R-squared from the Adjusted R-squared?*   
The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. It is always lower than the R-squared.   

***   
<br>

### 3. Can we predict future house values? How accurately? 

```{r}
predict(lm.fit, data.frame(lstat=(c(5,10,15))))
predict(lm.fit, data.frame(lstat=(c(5,10,15))), interval ="confidence")
predict(lm.fit, data.frame(lstat=(c(5,10,15))), interval ="prediction")
```

***   
**Note** *what's difference between confidence interval vs. prediction interval?*  
the confidence interval tells you about the likely location of the true population parameter. the prediction interval tells you about the distribution of values, not the uncertainty in determining the population mean. Prediction intervals will always be wider than confidence intervals because they account for the uncertainty associated with ε, the irreducible error.    

***    
<br> 

## Potential problems in linear regression  

### 1. Non-linearity of the data
<br>  

> Is the relationship linear?

```{r}
plot(lstat, medv)
abline(lm.fit)
```

```{r}
# diagnostic plots
par(mfrow=c(2,2))
plot(lm.fit)
```

```{r}
# linearity
# residual plots
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
abline(h=c(-3,3))
```

***   
**Note**  
Residual plots are a useful graphical tool for identifying non-linearity. The standardized (or studentized) residuals, computed by dividing each residual by its estimated standard error. If the residual plot indicates that there are non-linear associations in the data, then a simple approach is to use non-linear transformations of the predictors, such as logX, √X, and X^2, in the regression model.   

***    

> How can we improve the model? 

1. Polynomial  
2. Multiple  

polynomial linear regression  
```{r}
lm.fit2 = lm(medv ~ lstat + I(lstat^2))
summary(lm.fit2)
anova(lm.fit, lm.fit2)
# plot
library(ggplot2)
ggplot(Boston, aes(lstat, medv)) + geom_point() + 
  stat_smooth(method = "lm", col="blue") +
  stat_smooth(method="lm", formula = y ~ poly(x, 2), col="red")
```

multipe linear regression  
```{r}
lm.fit3 = lm(medv ~ lstat + age, data = Boston)
summary(lm.fit3)
lm.fit4 = lm(medv ~ ., data = Boston)
summary(lm.fit4)
```


### 2. Correlation of error terms 
<br>
If in fact there is correlation among the error terms, then the estimated standard errors will tend to underestimate the true standard errors. Such correlations frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. In many cases, observations that are obtained at adjacent time points will have positively correlated errors. Many methods have been developed to properly take account of correlations in the error terms in time series data.


### 3. Non-constant variance of error terms 
<br>
One can identify non-constant variances in the errors, or heteroscedasticity, from the presence of a funnel shape in
the residual plot.
When faced with this problem, one possible solution is to trans-
form the response Y using a concave function such as log Y or $\sqrt{Y}$ . Such a transformation results in a greater amount of shrinkage of the larger responses, leading to a reduction in heteroscedasticity.


### 4. Outliers 
<br>
If we believe that an outlier has occurred due to an error in data collection or recording, then one solution is to simply remove the observation. However, care should be taken, since an outlier may instead indicate a deficiency with the model, such as a missing predictor.

### 5. High leverage points  
<br>
Observations with high leverage have an unusual value for $x_i$


### 6. Collinearity  
<br>
Collinearity refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. Since collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for $\hat{β}_j$ to grow. 

***
**Note**   
A simple way to detect collinearity is to look at the correlation matrix of the predictors. Instead of inspecting the correlation matrix, a better way to assess multi-collinearity is to compute the variance inflation factor (VIF). 

$$VIF = \frac{var(\hat{β}_j)_f}{var(\hat{β}_j)_u}$$$ 
- f: full model  
- u; univariate model 
   
As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity.   

***
<br>

```{r}
library(car)
vif(lm.fit4) # variance inflation factor
# vif, cut-off for collinea rity
# what would be the solution for collinearity problem? 
lm.fit = update(lm.fit, ~. -age)
summary(lm.fit)
```


## Other considerations 

### 1. interaction terms 


```{r}
lm.fit = lm(medv ~ lstat*age, data = Boston)
ggplot(Boston, aes(lstat, medv, col = age)) + geom_point()
Boston$age_gr = cut(Boston$age, breaks = c(0,40,80,100), include.lowest = T, right = F)
levels(Boston$age_gr)
ggplot(Boston, aes(lstat, medv, col = age_gr)) + geom_point() + stat_smooth(method = "lm", se = F)
```

### 2. Qualitative predictors 

```{r}
Boston$chas = factor(Boston$chas)
lm.fit5 = lm(medv~., data = Boston)
summary(lm.fit5)
contrasts(Boston$chas) # dummy variables 
```



# 분류(classification)   

classifier: assign a probability to each class  

> Examples of classification problem... 을 생각해보자... 

## logistic regression 

Logit function  
$$P(X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}$$

Odds   
$$\frac{P(X)}{1-P(X)} = e^{\beta_0+\beta_1X}$$
**odds: P(event)/P(nonevent)** 


Logit 
$$log(\frac{P(X)}{1-P(X)}) = \beta_0 + \beta_1X$$ 

> titanic survival classification    

```{r}
library(carData)
head(TitanicSurvival)
str(TitanicSurvival)
summary(TitanicSurvival)
```

```{r}
TitanicSurvival = TitanicSurvival[complete.cases(TitanicSurvival),]
```

fit logistic regression model   
```{r}
glm.fit = glm(survived ~., data = TitanicSurvival, family = "binomial")
```

```{r}
summary(glm.fit)
coef(glm.fit)
```

odds  
```{r}
exp(coef(glm.fit)[2])
```
male 인 경우 female 에 비해서, 생존/죽음 odds가 0.08배 (즉, 훨씬 낮다, 10배 이상), Odds ratio 개념... 


dummy variables  
```{r}
contrasts(TitanicSurvival$survived)
contrasts(TitanicSurvival$passengerClass)
```

prediction 
```{r}
glm.probs = predict(glm.fit, type = "response")
```

confusion matrix   
accuracy  
```{r}
glm.pred = rep("no", 1046)
glm.pred[glm.probs>0.5] = "yes"
table(glm.pred, TitanicSurvival$survived)
mean(glm.pred == TitanicSurvival$survived)
```

test error rate
```{r}
set.seed(1)
index = sample(1:1046, round(1046/7), replace = F)
train.glm = TitanicSurvival[index,]
test.glm = TitanicSurvival[-index,]
```

fit the model wit training data, and predict in test data
```{r}
glm.fit = glm(survived ~., data = train.glm, 
              family = "binomial")
glm.probs = predict(glm.fit, test.glm, type = "response")
```

confusion matrix  
accuray  
```{r}
glm.pred = rep("no", 1046-round(1046/7))
glm.pred[glm.probs>0.5] = "yes"
table(glm.pred, test.glm$survived)
mean(glm.pred == test.glm$survived)
```

## linear discrminant analysis 

Bayes theorem  
$$P(B|A) = \frac{P(A|B)\times P(B)}{P(A)}$$
Bayes classifier 
$$P(Y=k|X=x) = \frac{P(X=x|Y=k) \times P(Y=k)}{\sum_{l=1}^k P(X=x|Y=l)} $$ 

assume in LDA   
- normal distribution of P(X|Y=k)   
- common variance across different k class   

calculate posterior probability with...   
- P(Y=k): proportion   
- $\mu_k: mean(X)\ in\ class\ k$   
- $\sigma^2: variance$   

다음 식이 최대가 되는 클래스에 관측치 X=x 를 할당한다.   
$$\hat{\delta_k} = x\times\frac{\hat{\mu_k}}{\hat{\sigma}^2}-\frac{\hat{\mu_k}^2}{2\hat{\sigma}^2}+log(\hat{\pi_k})$$

**figure** 
<img src="img/LDA.png" style="border: #A9A9A9 1px solid; width:75%">

```{r}
library(MASS)
lda.fit = lda(survived~., data = train.glm)
lda.fit
plot(lda.fit)
```

```{r}
lda.pred = predict(lda.fit, test.glm)
names(lda.pred)
```

```{r}
table(lda.pred$class, test.glm$survived)
mean(lda.pred$class == test.glm$survived)
```

## k-nearest neighbor 

```{r}
library(class) # knn
```

dummy variables  
```{r}
library(dummies)
TitanicSurvival = cbind(TitanicSurvival, dummy(TitanicSurvival$sex), dummy(TitanicSurvival$passengerClass))
TitanicSurvival.dummy = TitanicSurvival[,c(1,3,5:9)]
```

```{r}
train.knn = TitanicSurvival.dummy[index,]
test.knn = TitanicSurvival.dummy[-index,]
```


```{r}
train.x = train.knn[,-1]
train.y = train.knn[,1]
test.x = test.knn[,-1]
test.y = test.knn[,1]
```

```{r}
knn.pred = knn(train = train.x, test = test.x, cl = train.y, k=3)
```

```{r}
table(knn.pred, test.y)
mean(knn.pred == test.y)
```


## ROC curve 

**figure**
<img src="img/ROC.png" style="border: #A9A9A9 1px solid; width:75%">

```{r}
probs.glm = predict(glm.fit, test.glm, type = "response")
```

Load the ROCR library
```{r}
library(ROCR)
```

Make a prediction object: pred
```{r}
pred = prediction(probs.glm, test.glm$survived)
```

Make a performance object: perf
```{r}
perf1 = performance(pred, "tpr", "fpr")
```

Plot ROC curve
```{r}
plot(perf1)
```

Print out the AUC
```{r}
perf2 = performance(pred, "auc")
perf2@y.values[[1]]
```


