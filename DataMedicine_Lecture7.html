<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Decision tree and Random forest</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/paper.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="styles.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 64px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h2 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h3 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h4 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h5 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h6 {
  padding-top: 69px;
  margin-top: -69px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<div class="container-fluid main-container">

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Tutorial</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="https://yoonhohong.github.io">by Yoon H. Hong</a>
</li>
<li>
  <a href="http://www.github.com/yoonhohong/tutorial">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Decision tree and Random forest</h1>

</div>


<p><a href="https://www.youtube.com/watch?v=7VeUPuFGJHk">Decision tree</a></p>
<div id="decision-trees-regression" class="section level1">
<h1>Decision trees: regression</h1>
<p>설명변수 공간을 다수의 영역으로 분할하는 방법<br />
해당 관측치가 속하는 영역의 훈련 관측치들의 평균 (회귀) 또는 최빈값 (분류)을 사용하여 예측한다.</p>
<p><메이저리그 타자의 연봉을 예측하는 문제><br />
<img src="img/tree1.png" style="border: #A9A9A9 1px solid; width:75%"></p>
<p><img src="img/treeRegression.png" style="border: #A9A9A9 1px solid; width:75%"></p>
<p><em>advantage</em><br />
- glass-box model<br />
- intuitive</p>
<p>회귀 문제에서는 아래 식으로 주어진 RSS 를 최소로 하는 셜명변수 공간을 찾는 것이 목적</p>
<p><span class="math display">\[\sum_{j=1}^J\sum_{i\in R_j}(y_i - \hat{y}_{R_j})^2\]</span></p>
<p><span class="math inline">\(R_j\)</span>: jth 설명변수 공간<br />
<span class="math inline">\(\hat{y}_{R_j}\)</span>: jth 설명변수 공간에 속한 훈련 관측치 반응변수들의 평균값</p>
<p>그러나, 설명변수 공간을 J개로 분할하는 모든 가능한 경우를 고려하는 것은 계산상 실현이 불가능하다.</p>
<p><strong>recursive binary splitting</strong><br />
- top-down<br />
- greedy</p>
<p><img src="img/recursiveBinarySpliting.png" style="border: #A9A9A9 1px solid; width:75%"></p>
<p><em>drawbacks</em><br />
- greedy (short-sighted)<br />
- prone to overfitting</p>
<p><em>What “greedy” means?</em><br />
Let’s say currency including 25c, 15c and 1c coins,<br />
and develop an algorithm to make changes for 30c.<br />
final goal: using the smallest number of coins possible<br />
greedy algorithm: use the largest unit of coin<br />
greedy algorithm solution: 25 + 1 + 1 + 1 + 1 + 1 = 30c, with 6 coins<br />
optimal solution: 15 + 15 = 30c, with 2 coins</p>
<p><strong>strategies to overcome drawbacks</strong><br />
<em>pruning</em><br />
: 트리빌딩 초기 쓸모 없어 보이는 분할 이후에 아주 좋은 분할이 올 수 있다. 따라서, 더 나은 전략은 아주 큰 트리를 만든 다음에 그것을 다시 prune하여 subtree 를 얻는 것이다. 그러나, 이렇게 모든 가능한 subtree 에 대해 교차 검증(혹은 검증셋 기법)을 이용하여 검정오차율를 추청하는 것은 너무 번거롭다. 대신, 우리는 고려할 작은 subtree 집합을 다음 알고리즘으로 선택할 것이다.</p>
<p><strong>cost complexity pruning (weakest link pruning)</strong><br />
모든 가능한 subtree 를 고려하는 대신에 tuning parameter <span class="math inline">\(\alpha\)</span> (&gt;=0) 에 의해 색인된 일련의 tree 들을 고려.</p>
<p>각 <span class="math inline">\(\alpha\)</span> 값에 대해, 아래 식이 최소가 되는 subtree T 를 구할 수 있다.</p>
<p><span class="math display">\[\sum_{j=1}^J\sum_{i\in R_j}(y_i - \hat{y}_{R_j})^2 + \alpha T\]</span></p>
<p>T: subtree T 의 number of terminal nodes<br />
<span class="math inline">\(\alpha\)</span>: tuning parameter</p>
<p><span class="math inline">\(\alpha = 0\)</span>일 때, subtree T = <span class="math inline">\(T_0\)</span><br />
<span class="math inline">\(\alpha\)</span>가 증가함에 따라 많은 터미널 노드가 있는 트리의 경우 <span class="math inline">\(\alpha T\)</span> 항이 크게 증가할 것이므로 트리가 작을 때 위 식의 값이 최소로 되는 경향이 있다.<br />
즉, tuning parameter <span class="math inline">\(\alpha\)</span> 는 서브트리의 복잡도와 훈련자료에 대한 적합 사이의 trade-off 를 제어한다.</p>
<p>k-fold CV 을 이용해서 검정오차를 <span class="math inline">\(\alpha\)</span>의 함수로 평가하고, 평균 검정오차를 최소로 하는 <span class="math inline">\(\alpha\)</span>를 선택.</p>
<p><img src="img/MSE_pruning.png" style="border: #A9A9A9 1px solid; width:75%"></p>
<p>Linear model vs. Tree-based model</p>
<p><img src="img/linearVsTree.png" style="border: #A9A9A9 1px solid; width:75%"></p>
</div>
<div id="decision-trees-classification" class="section level1">
<h1>Decision trees: classification</h1>
<p><span class="math inline">\(\hat{p}_{mk}\)</span>를 m번째 설명변수 공간 내 k class에 속하는 훈련 관측치들의 비율이라고 할 때, 아래 분류오류율을 최소로 하는 것이 목적이다.</p>
<p><span class="math display">\[E = 1 - \max(\hat{p}_{mk})\]</span></p>
<p>binary split 과정에서 분류오류율을 사용할 수 있지만, 실제로는 node purity 에 더 민감한 아래 두 가지 척도를 주로 사용한다.</p>
<ul>
<li>지니 지수(Gini index)<br />
</li>
<li>교차엔트로피(cross-entropy)</li>
</ul>
<p>Gini index: node purity <span class="math display">\[G = \sum_{k=1}^{K} \hat{p}_{mk}(1-\hat{p}_{mk})\]</span></p>
<p>Cross-entropy<br />
<span class="math display">\[D = - \sum_{k=1}^{K} \hat{p}_{mk}log(\hat{p}_{mk})\]</span></p>
</div>
<div id="bagging" class="section level1">
<h1>Bagging</h1>
<p>a.k.a. bootstrap aggregation<br />
기계학습모델의 분산을 줄여 예측 정확도를 증가시키기 위한 범용 절차 (general-purpose procedure)</p>
<p><span class="math display">\[\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^{B}\hat{f}_b(x)\]</span> average for regression<br />
majority rule for classification</p>
<p><img src="img/bagging.png" style="border: #A9A9A9 1px solid; width:75%"></p>
<p>배깅에 사용되지 않은 관측치들을 Out-of-bag (OOB) 관측치라고 함.<br />
<strong>OOB 오차</strong><br />
: i번째 관측치에 대해 그 관측치가 OOB 이었던 각각의 트리를 이용하여 반응변수 값을 예측할 수 있다.<br />
: 교차검증 또는 검증셋 기법을 수행하기 힘든 규모가 큰 데이터셋에 대해 특히 편리함.</p>
<p><strong>Variable importance</strong> 주어진 설명변수에 대한 분할로 RSS (or Gini index) 가 감소되는 총량을 모든 B개 트리에 대해 평균한 값이 크면 해당 설명변수가 중요하다고 할 수 있다.</p>
</div>
<div id="random-forest" class="section level1">
<h1>Random Forest</h1>
<p>배깅에서와 마찬가지로 bootstrap 에 의해 다수의 트리를 만든다. 그러나, 배깅과 달리 트리 내에서 split 이 고려될 때마다 p개 설명변수들의 전체 집합에서 랜덤하게 m개 설명변수가 선택된다. rule of thumb, <span class="math inline">\(m = \sqrt{p}\)</span></p>
<p>결국, 트리들 간의 상관성을 줄여, 분산을 줄이는 방법임.</p>
<p><img src="img/randomForest.png" style="border: #A9A9A9 1px solid; width:75%"></p>
<p>자, 이제 실습을 해보자.</p>
<pre class="r"><code>library(MASS)
?Boston</code></pre>
<p>medv: median value of owner-occupied homes in $1K</p>
<pre class="r"><code>library(caret)</code></pre>
<pre class="r"><code>set.seed(1) 
train_index = createDataPartition(1:nrow(Boston), p=0.75, list = FALSE)
train_boston = Boston[train_index,]
test_boston = Boston[-train_index,]</code></pre>
<p>set resampling method</p>
<pre class="r"><code>myTrCtrl = trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 2)</code></pre>
<p>build model by using training data</p>
<pre class="r"><code>set.seed(999)
rf_model = train(medv ~ ., data = train_boston, method = &quot;rf&quot;, trControl = myTrCtrl, importance = TRUE)
rf_model</code></pre>
<pre><code>## Random Forest 
## 
## 382 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 2 times) 
## Summary of sample sizes: 343, 343, 345, 345, 344, 344, ... 
## Resampling results across tuning parameters:
## 
##   mtry  RMSE      Rsquared   MAE     
##    2    3.421202  0.8778653  2.357988
##    7    3.070891  0.8903573  2.119448
##   13    3.185884  0.8776762  2.198186
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was mtry = 7.</code></pre>
<pre class="r"><code>plot(rf_model)</code></pre>
<p><img src="DataMedicine_Lecture7_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>mtry: Number of variables randomly sampled as candidates at each split. Note that the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3)</p>
<p>test set 에서의 성능</p>
<pre class="r"><code>pred = predict(rf_model, newdata = test_boston)
plot(pred, test_boston$medv)
abline(0,1)</code></pre>
<p><img src="DataMedicine_Lecture7_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>sqrt(mean((pred - test_boston$medv)^2))</code></pre>
<pre><code>## [1] 3.928236</code></pre>
<pre class="r"><code>rf_imp = varImp(rf_model, scale = F)</code></pre>
<pre class="r"><code>plot(rf_imp)</code></pre>
<p><img src="DataMedicine_Lecture7_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="boosting" class="section level1">
<h1>Boosting</h1>
<p>여러개의 decision tree 를 만들어 결합하는데, 배깅과 달리 bootstrap 샘플링을 하지 않고, 대신 모든 훈련셋 자료를 이용하여 순차적으로 천천히 학습한다.</p>
<p>Regression decision tree boosting</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\hat{f}(x)\)</span> = 0 이라 하고, 훈련셋의 모든 i에 대해 <span class="math inline">\(r_i = y_i\)</span> 로 설정한다. (r: residuals)</p></li>
<li><p>b = 1,2,…,B 에 대하여 다음을 반복한다.</p></li>
</ol>
<ul>
<li><p>d개의 분할(d+1 터미널 노드)을 가진 트리 <span class="math inline">\(\hat{f}^b\)</span>를 훈련자료 (X,r)에 적합한다.</p></li>
<li><p>새로운 트리의 수축 버전을 더하여 <span class="math inline">\(\hat{f}(x)\)</span> 를 업데이트한다.</p></li>
</ul>
<p><span class="math display">\[\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)\]</span></p>
<ul>
<li>잔차들을 업데이트한다.</li>
</ul>
<p><span class="math display">\[r_i \leftarrow r_i - \lambda \hat{f}^b(x_i)\]</span></p>
<ol start="3" style="list-style-type: decimal">
<li>부스팅 모델을 출력한다.</li>
</ol>
<p><span class="math display">\[\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x)\]</span></p>
<p>Boosting 의 tuning parameters<br />
- B: number of trees<br />
- <span class="math inline">\(\lambda\)</span>: 수축 파라미터 (학습 속도를 제어)<br />
- d: number of split in each tree (boosting 의 복잡도를 제어)</p>
<p><img src="img/boosting.png" style="border: #A9A9A9 1px solid; width:75%"></p>
<p>자, 바로 Boston 데이터셋에서 실습을 해봅시다.</p>
<pre class="r"><code>set.seed(999)
gbm_model = train(medv~., data = train_boston, trControl = myTrCtrl, method = &quot;gbm&quot;, verbose = F) 
gbm_model</code></pre>
<pre><code>## Stochastic Gradient Boosting 
## 
## 382 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 2 times) 
## Summary of sample sizes: 343, 343, 345, 345, 344, 344, ... 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  RMSE      Rsquared   MAE     
##   1                   50      3.901649  0.8233716  2.725915
##   1                  100      3.633085  0.8452395  2.496197
##   1                  150      3.530275  0.8557658  2.415587
##   2                   50      3.569309  0.8507147  2.463461
##   2                  100      3.389451  0.8666858  2.388910
##   2                  150      3.306348  0.8724642  2.346318
##   3                   50      3.383426  0.8671826  2.319509
##   3                  100      3.179842  0.8828976  2.237975
##   3                  150      3.116506  0.8872130  2.214115
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were n.trees = 150,
##  interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>interaction.depth: the maximum depth of each tree, i.e., the highest level of variable interactions allowed n.trees: number of trees to fit<br />
shrinkage: learning rate<br />
n.minobsinnode: the mininum number of observations in the terminal nodes of the trees</p>
<pre class="r"><code>plot(gbm_model)</code></pre>
<p><img src="DataMedicine_Lecture7_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>gbmGrid &lt;-  expand.grid(interaction.depth = c(1, 5,10), n.trees = c(50*(1:10)), shrinkage = 0.1, n.minobsinnode = 10)</code></pre>
<pre class="r"><code>summary(gbm_model, las = 1)</code></pre>
<p><img src="DataMedicine_Lecture7_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre><code>##             var     rel.inf
## rm           rm 44.01473135
## lstat     lstat 30.88274794
## dis         dis  6.16679041
## nox         nox  4.88548338
## crim       crim  4.59543773
## ptratio ptratio  3.30824158
## age         age  1.88598637
## tax         tax  1.47300121
## black     black  1.23655499
## indus     indus  0.81033147
## rad         rad  0.43222105
## chas       chas  0.21724337
## zn           zn  0.09122915</code></pre>
<pre class="r"><code>gbm_pred = predict(gbm_model, newdata = test_boston)
sqrt(mean(gbm_pred - test_boston$medv)^2)</code></pre>
<pre><code>## [1] 0.4583329</code></pre>
<pre class="r"><code>resamps = resamples(list(RF = rf_model, 
               GBM = gbm_model))
resamps</code></pre>
<pre><code>## 
## Call:
## resamples.default(x = list(RF = rf_model, GBM = gbm_model))
## 
## Models: RF, GBM 
## Number of resamples: 20 
## Performance metrics: MAE, RMSE, Rsquared 
## Time estimates for: everything, final model fit</code></pre>
<pre class="r"><code>summary(resamps)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = resamps)
## 
## Models: RF, GBM 
## Number of resamples: 20 
## 
## MAE 
##         Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## RF  1.470750 1.797423 2.068841 2.119448 2.439328 2.676035    0
## GBM 1.758432 1.966517 2.216221 2.214115 2.318802 2.778140    0
## 
## RMSE 
##         Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## RF  2.183433 2.399354 2.775105 3.070891 3.441502 5.043234    0
## GBM 2.282409 2.690868 2.973839 3.116506 3.473812 4.807602    0
## 
## Rsquared 
##          Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## RF  0.7655401 0.8721144 0.9158214 0.8903573 0.9307836 0.9431408    0
## GBM 0.7752630 0.8630659 0.8927498 0.8872130 0.9191522 0.9532604    0</code></pre>
<pre class="r"><code>bwplot(resamps)</code></pre>
<p><img src="DataMedicine_Lecture7_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
