---
title: "Decision tree and Random forest"
output:
  html_document:
    toc: yes
    toc_float: true
editor_options: 
  chunk_output_type: console
---

[Decision tree](https://www.youtube.com/watch?v=7VeUPuFGJHk)  

# Decision trees: regression  

설명변수 공간을 다수의 영역으로 분할하는 방법   
해당 관측치가 속하는 영역의 훈련 관측치들의 평균 (회귀) 또는 최빈값 (분류)을 사용하여 예측한다.  

<메이저리그 타자의 연봉을 예측하는 문제>    
<img src="img/tree1.png" style="border: #A9A9A9 1px solid; width:75%">

<img src="img/treeRegression.png" style="border: #A9A9A9 1px solid; width:75%">

*advantage*  
- glass-box model   
- intuitive   


회귀 문제에서는 아래 식으로 주어진 RSS 를 최소로 하는 셜명변수 공간을 찾는 것이 목적   

$$\sum_{j=1}^J\sum_{i\in R_j}(y_i - \hat{y}_{R_j})^2$$

$R_j$: jth 설명변수 공간  
$\hat{y}_{R_j}$: jth 설명변수 공간에 속한 훈련 관측치 반응변수들의 평균값   


그러나, 설명변수 공간을 J개로 분할하는 모든 가능한 경우를 고려하는 것은 계산상 실현이 불가능하다.   

**recursive binary splitting**  
- top-down   
- greedy   


<img src="img/recursiveBinarySpliting.png" style="border: #A9A9A9 1px solid; width:75%">

*drawbacks*  
- greedy (short-sighted)     
- prone to overfitting   

*What "greedy" means?*  
Let's say currency including 25c, 15c and 1c coins,    
and develop an algorithm to make changes for 30c.     
final goal: using the smallest number of coins possible   
greedy algorithm: use the largest unit of coin    
greedy algorithm solution: 25 + 1 + 1 + 1 + 1 + 1 = 30c, with 6 coins   
optimal solution: 15 + 15 = 30c, with 2 coins   


**strategies to overcome drawbacks**   
*pruning*   
: 트리빌딩 초기 쓸모 없어 보이는 분할 이후에 아주 좋은 분할이 올 수 있다. 따라서, 더 나은 전략은 아주 큰 트리를 만든 다음에 그것을 다시 prune하여 subtree 를 얻는 것이다. 그러나, 이렇게 모든 가능한 subtree 에 대해 교차 검증(혹은 검증셋 기법)을 이용하여 검정오차율를 추청하는 것은 너무 번거롭다. 대신, 우리는 고려할 작은 subtree 집합을 다음 알고리즘으로 선택할 것이다.  

**cost complexity pruning (weakest link pruning)**    
모든 가능한 subtree 를 고려하는 대신에 tuning parameter $\alpha$ (>=0) 에 의해 색인된 일련의 tree 들을 고려.    

각 $\alpha$ 값에 대해, 아래 식이 최소가 되는 subtree T 를 구할 수 있다.  

$$\sum_{j=1}^J\sum_{i\in R_j}(y_i - \hat{y}_{R_j})^2 + \alpha T$$ 

T: subtree T 의 number of terminal nodes   
$\alpha$: tuning parameter   

$\alpha = 0$일 때, subtree T = $T_0$   
$\alpha$가 증가함에 따라 많은 터미널 노드가 있는 트리의 경우 $\alpha T$ 항이 크게 증가할 것이므로 트리가 작을 때 위 식의 값이 최소로 되는 경향이 있다.   
즉, tuning parameter $\alpha$ 는 서브트리의 복잡도와 훈련자료에 대한 적합 사이의 trade-off 를 제어한다.   


k-fold CV 을 이용해서 검정오차를 $\alpha$의 함수로 평가하고, 평균 검정오차를 최소로 하는 $\alpha$를 선택.   

<img src="img/MSE_pruning.png" style="border: #A9A9A9 1px solid; width:75%">

Linear model vs. Tree-based model 

<img src="img/linearVsTree.png" style="border: #A9A9A9 1px solid; width:75%">


# Decision trees: classification  

$\hat{p}_{mk}$를 m번째 설명변수 공간 내 k class에 속하는 훈련 관측치들의 비율이라고 할 때, 아래 분류오류율을 최소로 하는 것이 목적이다.    

$$E = 1 - \max(\hat{p}_{mk})$$

binary split 과정에서 분류오류율을 사용할 수 있지만, 실제로는 node purity 에 더 민감한 아래 두 가지 척도를 주로 사용한다.     

- 지니 지수(Gini index)  
- 교차엔트로피(cross-entropy)   

Gini index: node purity 
$$G = \sum_{k=1}^{K} \hat{p}_{mk}(1-\hat{p}_{mk})$$

Cross-entropy   
$$D = - \sum_{k=1}^{K} \hat{p}_{mk}log(\hat{p}_{mk})$$  


# Bagging 

a.k.a. bootstrap aggregation   
기계학습모델의 분산을 줄여 예측 정확도를 증가시키기 위한 범용 절차 (general-purpose procedure)   

$$\hat{f}_{bag}(x) = \frac{1}{B}\sum_{b=1}^{B}\hat{f}_b(x)$$ 
average for regression   
majority rule for classification   

<img src="img/bagging.png" style="border: #A9A9A9 1px solid; width:75%">

배깅에 사용되지 않은 관측치들을 Out-of-bag (OOB) 관측치라고 함.   
**OOB 오차**  
: i번째 관측치에 대해 그 관측치가 OOB 이었던 각각의 트리를 이용하여 반응변수 값을 예측할 수 있다.  
: 교차검증 또는 검증셋 기법을 수행하기 힘든 규모가 큰 데이터셋에 대해 특히 편리함.   

**Variable importance** 
주어진 설명변수에 대한 분할로 RSS (or Gini index) 가 감소되는 총량을 모든 B개 트리에 대해 평균한 값이 크면 해당 설명변수가 중요하다고 할 수 있다.   

# Random Forest 

[Random Forest part I](https://www.youtube.com/watch?v=J4Wdy0Wc_xQ)
[Random Forest part II](https://www.youtube.com/watch?v=nyxTdL_4Q-Q) 

배깅에서와 마찬가지로 bootstrap 에 의해 다수의 트리를 만든다. 그러나, 배깅과 달리 트리 내에서 split 이 고려될 때마다 p개 설명변수들의 전체 집합에서 랜덤하게 m개 설명변수가 선택된다. 
rule of thumb, $m = \sqrt{p}$   

결국, 트리들 간의 상관성을 줄여, 분산을 줄이는 방법임.  

<img src="img/randomForest.png" style="border: #A9A9A9 1px solid; width:75%">

자, 이제 실습을 해보자.  

```{r message=FALSE}
library(MASS)
?Boston
```
medv: median value of owner-occupied homes in  $1K  

```{r message=FALSE}
library(caret)
```

```{r}
set.seed(1) 
train_index = createDataPartition(1:nrow(Boston), p=0.75, list = FALSE)
train_boston = Boston[train_index,]
test_boston = Boston[-train_index,]
```

set resampling method 
```{r}
myTrCtrl = trainControl(method = "repeatedcv", number = 10, repeats = 2)
```

build model by using training data 
```{r}
set.seed(999)
rf_model = train(medv ~ ., data = train_boston, method = "rf", trControl = myTrCtrl, importance = TRUE)
rf_model
plot(rf_model)
```

mtry: Number of variables randomly sampled as candidates at each split. Note that the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3)


test set 에서의 성능
```{r}
pred = predict(rf_model, newdata = test_boston)
plot(pred, test_boston$medv)
abline(0,1)
sqrt(mean((pred - test_boston$medv)^2))
```


```{r}
rf_imp = varImp(rf_model, scale = F)
```

```{r}
plot(rf_imp)
```

# Boosting 



여러개의 decision tree 를 만들어 결합하는데, 배깅과 달리 bootstrap 샘플링을 하지 않고, 대신 모든 훈련셋 자료를 이용하여 순차적으로 천천히 학습한다.   

Regression decision tree boosting   

1. $\hat{f}(x)$ = 0 이라 하고, 훈련셋의 모든 i에 대해 $r_i = y_i$ 로 설정한다. (r: residuals) 

2. b = 1,2,...,B 에 대하여 다음을 반복한다.   

- d개의 분할(d+1 터미널 노드)을 가진 트리 $\hat{f}^b$를 훈련자료 (X,r)에 적합한다.   

- 새로운 트리의 수축 버전을 더하여 $\hat{f}(x)$ 를 업데이트한다.   

$$\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^b(x)$$

- 잔차들을 업데이트한다.   

$$r_i \leftarrow r_i - \lambda \hat{f}^b(x_i)$$

3. 부스팅 모델을 출력한다.   

$$\hat{f}(x) = \sum_{b=1}^B \lambda \hat{f}^b(x)$$

Boosting 의 tuning parameters   
- B: number of trees   
- $\lambda$: 수축 파라미터 (학습 속도를 제어)  
- d: number of split in each tree (boosting 의 복잡도를 제어) 

<img src="img/boosting.png" style="border: #A9A9A9 1px solid; width:75%">

자, 바로 Boston 데이터셋에서  실습을 해봅시다.   
```{r}
set.seed(999)
gbm_model = train(medv~., data = train_boston, trControl = myTrCtrl, method = "gbm", verbose = F) 
gbm_model
```

interaction.depth: the maximum depth of each tree, i.e., the highest level of variable interactions allowed
n.trees: number of trees to fit   
shrinkage: learning rate   
n.minobsinnode: the mininum number of observations in the terminal nodes of the trees    

```{r}
plot(gbm_model)
```

```{r}
gbmGrid <-  expand.grid(interaction.depth = c(1, 5,10), n.trees = c(50*(1:10)), shrinkage = 0.1, n.minobsinnode = 10)
```
                        
```{r}
summary(gbm_model, las = 1)
```

```{r}
gbm_pred = predict(gbm_model, newdata = test_boston)
sqrt(mean(gbm_pred - test_boston$medv)^2)
```

```{r}
resamps = resamples(list(RF = rf_model, 
               GBM = gbm_model))
resamps
summary(resamps)
bwplot(resamps)
```

