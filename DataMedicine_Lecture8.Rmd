---
title: "Regularization"
output:
  html_document:
    toc: yes
    toc_float: true
editor_options: 
  chunk_output_type: console
---

# Curse of dimensionality 

in circumstances of p >> n   
using all the features to predict response variable   
will face two drawbacks in terms of ...   
- interpretability  
- increase of variance (overfitting)   
</br>

so, we do   
feature selection   
- subset selection   
- shrinkage (regularization)   
- dimension reduction   

# Subset selection 

가능한 경우의 수: $2^p$  
stepwise selection: forward, backward   

# Shrinkage 

[Ridge regression](https://www.youtube.com/watch?v=Q81RR3yKn30)  [Lasso regression](https://www.youtube.com/watch?v=NGf0voTMlcs)  [Elastic-net regression](https://www.youtube.com/watch?v=1dKRdX9bfIo) 


아래 식을 최소로 하는 $\beta$ 를 추정 (note: shrinkage penalty)     
ridge regresssion  
$$RSS + \lambda \sum_{j=1}^{p} \beta^2_j$$  
$\lambda$: tuning parameter   

lasso regression   
$$RSS + \lambda \sum_{j=1}^{p} |\beta_j|$$   


```{r include=FALSE}
library(ISLR)
?Hitters
str(Hitters)
summary(Hitters)
Hitters = na.omit(Hitters)
```

```{r echo=TRUE}
library(glmnet)
x = model.matrix(Salary~., Hitters)[,-1] # create a matrix, convert factors to a set of dummy variables  
y = Hitters$Salary
```

```{r}
grid = 10^seq(10, -2, length = 100) # lambda 를 10^10 에서 10^-2 까지 값을 갖도록 설정 
```

```{r}
ridge.mod = glmnet(x,y,alpha = 0, lambda = grid) # alpha = 0 for ridge regression, alpha = 1 for lasso regression 
?glmnet # standardize = TRUE 
```

```{r}
plot(ridge.mod)
```

L2 norm: $\sqrt{\sum_{j=1}^{p}\beta^2_j}$ 

```{r eval = F}
coef(ridge.mod)
```

```{r eval=F}
ridge.mod$lambda
```

```{r}
ridge.mod$lambda[50]
coef.50lambda = coef(ridge.mod)[-1,50] # at 50th lambda, coefficients
sqrt(sum(coef.50lambda^2)) # L2 norm at 50th lambda 
```

```{r}
set.seed(1)
train = sample(nrow(x), nrow(x)/2)
y.test = y[-train]
```

arbitrary set lamda = 4   
```{r}
ridge.mod = glmnet(x[train,], y[train], alpha = 0, lambda = grid)
ridge.pred = predict(ridge.mod, s=4, newx = x[-train,])
mean((ridge.pred-y.test)^2)
```

cv 를 통해 검정오차 추정치가 가장 낮은 lambda 값을 구함   
```{r}
set.seed(1)
cv.out.ridge = cv.glmnet(x[train,], y[train], alpha = 0) # nfolds = 10 
plot(cv.out.ridge)
bestlambda.ridge = cv.out.ridge$lambda.min
bestlambda.ridge
```

cv 검정오차 추정치가 가장 낮은 lambda 값을 이용하여 검정셋에서 검정오차를 구함  
```{r}
ridge.pred = predict(ridge.mod, s=bestlambda.ridge, newx = x[-train,])
mean((ridge.pred - y.test)^2)
```

전체 데이터셋에서 ridge regression 모델을 만들고, 해당 coefficients 값을 구함   
```{r}
ridge.out = glmnet(x,y,alpha = 0)
predict(ridge.out, type = "coefficients", s=bestlambda.ridge)
```

lasso  
```{r}
lasso.mod = glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)
```

```{r}
set.seed(1)
cv.lasso.out = cv.glmnet(x[train,], y[train], alpha=1)
plot(cv.lasso.out)
bestlambda.lasso = cv.lasso.out$lambda.min
lasso.pred = predict(lasso.mod, s=bestlambda.lasso, newx = x[-train,])
mean((lasso.pred - y.test)^2)
```

```{r}
lasso.out = glmnet(x,y,alpha = 1, lambda = grid)
lasso.coef = predict(lasso.out, type = "coefficients", s=bestlambda.lasso)
lasso.coef
```

linear regression 
```{r}
lm.fit = lm(Salary~., data = Hitters, subset = train)
lm.pred = predict(lm.fit, newdata = Hitters[-train,])
mean((lm.pred - y.test)^2)
```


# Dimension reduction
- principal component regression (PCR)   
- partial least square (PLS)   
   
   
p개의 설명변수들을 m개의 (m<p) 새로운 변수로 변환하고, 변환된 변수들을 사용해 최소제곱모델을 적합하는 기법   

PCA (-> PCR)
: 설명변수들의 *정규화*된 선형결합  
: 분산이 가장 큰 방향 (첫번째 주성분) 

PLS
: PCA의 supervised version 
: 반응변수와 극대화된 상관관계를 갖도록 선형결합  


```{r}
library(pls)
set.seed(2)
pcr.fit = pcr(Salary~., data = Hitters, subset = train, scale = TRUE, validation = "CV")
summary(pcr.fit)
```

```{r}
validationplot(pcr.fit, val.type = "MSEP")
```

```{r}
pcr.pred = predict(pcr.fit, x[-train,], ncomp = 2)
mean((pcr.pred - y.test)^2)
```

```{r}
pcr.fit = pcr(y~x, scale = TRUE, ncomp = 2)
summary(pcr.fit)
```

PLS   
```{r}
set.seed(1)
pls.fit = plsr(Salary~., data = Hitters, subset = train, scale = TRUE, validation = "CV")
summary(pls.fit)
```

```{r}
validationplot(pls.fit, val.type = "MSEP")
```

```{r}
pls.pred = predict(pls.fit, x[-train,], ncomp = 2)
mean((pls.pred-y.test)^2)
```

```{r}
pls.fit=plsr(Salary~., data=Hitters, scale=TRUE, ncomp=2)
summary(pls.fit)
```
