---
title: "Unsupervised machine learning"
output:
  html_document:
    toc: yes
    toc_float: true
editor_options: 
  chunk_output_type: console
---

# Unsupervised Learning 

- Principal component analysis (PCA)  
- K-mean clustering   
- Hierchical clustering  

not interested in prediction   
then, for what?    

PCA
: data visualization and preprocessing for high-dimensional data   
   
   
clustering
: subgrouping of predictors or observations which may provide valuable insignts on the data   

## PCA 

the first principal component  
: 변수들의 정규화된 (normalized) 선형결합 (가장 큰 분산을 가지는, 즉 관측치가 가장 많이 변화되는 변수공간의 방향) 

$$Z_1 = \phi_{11}X_1 + \phi_{21}X_2 + ... + \phi_{p1}X_p$$ 

$$\sum_{j=1}^p \phi^2_{j1} = 1$$

loading vector (length=p)

$$\phi_1 = (\phi_{11}, \phi_{21},,,\phi_{p1})^T$$

principal component scores (length=n)

$$(z_{11}, z_{21},,, z_{n1})$$  


the second principal component  
: 첫번째 주성분과 상관되지 않은 (uncorrelated, orthogonal) $X_1, X_2,,, X_p$ 의 모든 선형결합 중에서 분산을 최대로 하는 선형결합   
  
  
변수 스케일링   
: 스케일링되지 않은 변수에 PCA 를 수행하면 단순히 분산이 큰 변수에 의해 주성분벡터가 결정됨   

```{r}
states = row.names(USArrests)
states
```

```{r}
names(USArrests)
```

```{r}
summary(USArrests)
```

```{r}
apply(USArrests, 2, var)
```

```{r}
pr.out = prcomp(USArrests, scale = TRUE)
names(pr.out)
pr.out$rotation # loading vectors 
pr.out$x # pc scores
```

```{r}
biplot(pr.out)
pr.out$rotation = -pr.out$rotation # loading vectors 
pr.out$x = -pr.out$x # pc scores
biplot(pr.out, scale = 0)
```

```{r}
pr.var = pr.out$sdev^2
pr.var
```

```{r}
pve = pr.var/sum(pr.var)
pve
```

```{r}
plot(pve, xlab = "Principal component", ylab = "Proportion of Variance Explained", type = "b") # scree plot 
```


```{r}
library(ISLR)
class(NCI60) # NCI60: 64개 암세포주에 대한 6830개의 유전자 발현 관측치 
names(NCI60)
nci.labs = NCI60$labs 
nci.labs
table(nci.labs) # 14개 유형 
```

```{r}
nci.data = NCI60$data
```

```{r}
pr.out = prcomp(nci.data, scale = TRUE) 
```

```{r}
library(ggplot2)
df = data.frame(pr.out$x[,c(1,2,3)], nci.labs)
ggplot(data = df, aes(x=PC1, y=PC2, col = nci.labs)) + geom_point(size = 3)
```

```{r}
summary(pr.out)
```


## K-means clustering 


n개 관측치들을 K개 클러스터로 분할하는 방법: $K^n$
모든 가능한 경우들 중에서 within cluster variation 을 가장 작게만드는 분할을 찾지 않고 (global optimum), 대신 국소 최적값(local optimum)을 제공하는 알고리즘을 사용한다.    

즉,  

1. 각 관측치에 1에서 K까지의 숫자를 랜덤하게 할당한다. 이것은 관측체에 대한 초기 클러스터 할당으로 작용한다. 

2. 클러스터 할당이 변하지 않을 때까지 다음을 반복한다. 

  + 2-1. K개 클러스터 각각에 대해 클러스터 무게중심을 계산한다 (p 변수 평균들의 벡터)  
  + 2-2. 각 관측치를 그 무게 중심이 가장 가까운 클러스터에 할당한다. 



## Hierachical clustering 

bottom-up or agglomerative   
dendrogram  

distance measures... 
- euclidean distance  
$$d_{euc}(x,y) = \sqrt{\sum_{i=1}^n(x_i - y_i)^2}$$ 

- manhattan distance   
$$d_{man}(x,y) = \sum_{i=1}^n |{(x_i - y_i)|}$$  

- pearson correlation distance   
$$d_{cor}(x, y) = 1 - \frac{\sum\limits_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum\limits_{i=1}^n(x_i - \bar{x})^2 \sum\limits_{i=1}^n(y_i -\bar{y})^2}}$$   

스케일링   
$$\frac{x_i - center(x)}{scale(x)}$$ 

linkage    
- complete linkage   
- average linkage   
- single linkage  
- centroid linkage   

```{r}
sd.data = scale(nci.data) # scaling 
```

```{r}
data.dist = dist(sd.data)
hclust.complete = hclust(data.dist, method = "complete")
plot(hclust.complete, labels = nci.labs, main = "complete linkage")
hc.clusters = cutree(hclust.complete, 5)
table(hc.clusters, nci.labs)
abline(h=135, col="red")
```

```{r}
set.seed(2)
km.out = kmeans(sd.data, 5, nstart = 20)
km.clusters = km.out$cluster
table(km.clusters, hc.clusters)
```

```{r}
pc.dist = dist(pr.out$x[,1:5])
hc.out = hclust(pc.dist)
plot(hc.out, labels = nci.labs, main = "Hier. Clust. on First Five Score Vectors")
hc.clusters = cutree(hc.out, 4)
table(hc.clusters, nci.labs)
```


```{r}
set.seed(3)
pc.scale = scale(pr.out$x[,1:5])
km.out = kmeans(pc.scale, 4, nstart = 20)
table(km.out$cluster, hc.clusters)
```

### Heatmap 

```{r}
df.scaled = scale(USArrests)
df.dist = dist(df.scaled) # row-wise distance matrix 
hc.out = hclust(df.dist, method = "complete")
plot(hc.out)
```

```{r}
hc.clusters = cutree(hc.out, 4)
cluster.membership = hc.clusters 
```

load package
```{r}
library(pheatmap)
```

```{r}
mat = as.matrix(t(df.scaled))
apply(mat, 1, sd)
```
```{r}
pheatmap(mat, 
         cluster_rows = T,
         # cellheight = 20,
         annotation_col = data.frame(cluster.membership))
```






